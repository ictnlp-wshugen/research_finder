 1: Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches
 2: Fusing Recency into Neural Machine Translation with an Inter-Sentence Gate Model
 3: Improving Neural Machine Translation by Incorporating Hierarchical Subword Features
 4: A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation
 5: On Adversarial Examples for Character-Level Neural Machine Translation
 6: Refining Source Representations with Relation Networks for Neural Machine Translation
 7: A Survey of Domain Adaptation for Neural Machine Translation
 8: An Evaluation of Neural Machine Translation Models on Historical Spelling Normalization
 9: Incorporating Syntactic Uncertainty in Neural Machine Translation with a Forest-to-Sequence Model
10: Neural Machine Translation with Decoding History Enhanced Attention
11: Multi-layer Representation Fusion for Neural Machine Translation
12: Adaptive Weighting for Neural Machine Translation
13: Multilingual Neural Machine Translation with Task-Specific Attention
14: Sentence Weighting for Neural Machine Translation Domain Adaptation
15: Neural Machine Translation Incorporating Named Entity
16: Deconvolution-Based Global Decoding for Neural Machine Translation
